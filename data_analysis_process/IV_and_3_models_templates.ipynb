{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IV"
      ],
      "metadata": {
        "id": "BSAeuivMSM8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th3BJ0fbCsjt"
      },
      "outputs": [],
      "source": [
        "from optbinning import OptimalBinning\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "iv_df = []\n",
        "\n",
        "for i in numerical_list: # 'categorical_list' if categorical variables\n",
        "    variable = i\n",
        "    x = df[variable].values\n",
        "    y = df.credit\n",
        "\n",
        "    # numerical\n",
        "    optb = OptimalBinning(name=variable, dtype='numerical', solver='cp',\n",
        "                          max_n_prebins=3) # bin이 많아지면 해석하기 어려우므로 3개가 적당함.\n",
        "    # categorical\n",
        "    # optb = OptimalBinning(name=variable, dtype='categorical', solver='cp')\n",
        "    optb.fit(x,y) # fitting을 시켜서 사용하는 경우가 대부분.\n",
        "\n",
        "    binning_table = optb.binning_table\n",
        "    v1 = binning_table.build()\n",
        "\n",
        "    loop_df = pd.DataFrame({'val' : variable,\n",
        "                            'IV' : [v1.loc['Totals', 'IV']]})\n",
        "    iv_df.append(loop_df)\n",
        "\n",
        "iv_df = pd.concat(iv_df).reset_index(drop=True)\n",
        "iv_df.sort_values(by=['IV'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bins 활용 labeling\n",
        "x_transform_bins = optb.transform(df['DAYS_BIRTH'], metric='bins')\n",
        "df_bin = pd.DataFrame(df['credit'])\n",
        "df_bin['bin'] = x_transform_bins\n",
        "df_bin.head()"
      ],
      "metadata": {
        "id": "a4IPTABuRV8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 models"
      ],
      "metadata": {
        "id": "eLDQnep_SGaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# X & Y\n",
        "X = df.drop(['index', 'credit', 'FLAG_MOBIL'], axis=1)\n",
        "Y = df['credit']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.3, stratify=Y, random_state=1234)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# onehot encoding\n",
        "for col in categorical_list:\n",
        "    encoder = OneHotEncoder()\n",
        "    encoder.fit(x_train[[col]])\n",
        "    onehot_train = pd.DataFrame(encoder.transform(x_train[[col]]).toarray(), columns=encoder.get_feature_names_out(), index=x_train.index)\n",
        "    onehot_test = pd.DataFrame(encoder.transform(x_test[[col]]).toarray(), columns=encoder.get_feature_names_out(), index=x_test.index)\n",
        "    # 기존 col 삭제\n",
        "    x_train = pd.concat([x_train, onehot_train], axis=1).drop(columns=[col])\n",
        "    x_test = pd.concat([x_test, onehot_test], axis=1).drop(columns=[col])\n",
        "\n",
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "x_train_sc = scaler.fit_transform(x_train)\n",
        "x_test_sc = scaler.transform(x_test)\n",
        "\n",
        "# model\n",
        "LR = LogisticRegression()\n",
        "LR.fit(x_train_sc, y_train)\n",
        "\n",
        "y_pred_train = LR.predict(x_train_sc)\n",
        "y_pred_test = LR.predict(x_test_sc)\n",
        "\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "# 과적합 문제, train과 test set에 성능을 최대한 줄여주는 것이 과적합을 방지\n",
        "y_pred_train_proba = LR.predict_proba(x_train_sc)[:, 1]\n",
        "y_pred_test_proba = LR.predict_proba(x_test_sc)[:, 1]\n",
        "\n",
        "roc_score_train = roc_auc_score(y_train, y_pred_train_proba)\n",
        "roc_score_test = roc_auc_score(y_test, y_pred_test_proba)\n",
        "\n",
        "print('roc_score_train: ', roc_score_train)\n",
        "print('roc_score_test: ', roc_score_test)\n",
        "\n",
        "lr_re = pd.DataFrame({'model' : ['LR'],\n",
        "                      'f1_train' : metrics.f1_score(y_train, y_pred_train),\n",
        "                      'f1_test' : metrics.f1_score(y_test, y_pred_test),\n",
        "                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),\n",
        "                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba)})\n",
        "\n",
        "df_comparison = df_comparison.append(lr_re)\n",
        "df_comparison"
      ],
      "metadata": {
        "id": "JM0Cm13YSOYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random forest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = df.drop(['index', 'credit', 'FLAG_MOBIL'], axis=1)\n",
        "Y = df['credit']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.3, stratify=Y, random_state=1234)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# model\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "y_pred_train = rfc.predict(x_train)\n",
        "y_pred_test = rfc.predict(x_test)\n",
        "\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# 과적합 문제, train과 test set에 성능을 최대한 줄여주는 것이 과적합을 방지\n",
        "y_pred_train_proba = rfc.predict_proba(x_train)[:, 1]\n",
        "y_pred_test_proba = rfc.predict_proba(x_test)[:, 1]\n",
        "\n",
        "roc_score_train = roc_auc_score(y_train, y_pred_train_proba)\n",
        "roc_score_test = roc_auc_score(y_test, y_pred_test_proba)\n",
        "\n",
        "print('roc_score_train: ', roc_score_train)\n",
        "print('roc_score_test: ', roc_score_test)\n",
        "\n",
        "# !pip install -q bayesian-optimization bayesian-optimization 패키지\n",
        "\n",
        "def model_evaluate(n_estimators, maxDepth):\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators = int(n_estimators),\n",
        "        max_depth = int(maxDepth)\n",
        "    )\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return np.mean(scores)\n",
        "\n",
        "def bayesOpt(x_train, y_train):\n",
        "    clfBO = BayesianOptimization(model_evaluate, {'n_estimators' : (100,200),\n",
        "                                                  'maxDepth' : (2,4)})\n",
        "    clfBO.maximize(init_points=5, n_iter=10)\n",
        "    print(clfBO.res)\n",
        "\n",
        "bayesOpt(x_train, y_train)\n",
        "\n",
        "# after finding out best parameters\n",
        "rfc = RandomForestClassifier(n_estimators=147,\n",
        "                             max_depth=4,\n",
        "                             random_state=1121)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "y_pred_train = rfc.predict(x_train)\n",
        "y_pred_test = rfc.predict(x_test)\n",
        "\n",
        "y_pred_train_proba = rfc.predict_proba(x_train)[:, 1]\n",
        "y_pred_test_proba = rfc.predict_proba(x_test)[:, 1]\n",
        "\n",
        "rfc_re = pd.DataFrame({'model' : ['RFC(BO)'],\n",
        "                      'f1_train' : metrics.f1_score(y_train, y_pred_train),\n",
        "                      'f1_test' : metrics.f1_score(y_test, y_pred_test),\n",
        "                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),\n",
        "                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba)})\n",
        "\n",
        "df_comparison = df_comparison.append(rfc_re)\n",
        "df_comparison.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "HePl270PSkcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LGBM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = df.drop(['index', 'credit', 'FLAG_MOBIL'], axis=1)\n",
        "Y = df['credit']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.3, stratify=Y, random_state=1234)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# model\n",
        "LGBM = LGBMClassifier()\n",
        "LGBM.fit(x_train, y_train)\n",
        "\n",
        "y_pred_train = LGBM.predict(x_train)\n",
        "y_pred_test = LGBM.predict(x_test)\n",
        "\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# 과적합 문제, train과 test set에 성능을 최대한 줄여주는 것이 과적합을 방지\n",
        "y_pred_train_proba = LGBM.predict_proba(x_train)[:, 1]\n",
        "y_pred_test_proba = LGBM.predict_proba(x_test)[:, 1]\n",
        "\n",
        "roc_score_train = roc_auc_score(y_train, y_pred_train_proba)\n",
        "roc_score_test = roc_auc_score(y_test, y_pred_test_proba)\n",
        "\n",
        "print('roc_score_train: ', roc_score_train)\n",
        "print('roc_score_test: ', roc_score_test)\n",
        "\n",
        "# !pip install -q bayesian-optimization bayesian-optimization 패키지\n",
        "\n",
        "def model_evaluate(n_estimators, maxDepth):\n",
        "    clf = LGBMClassifier(\n",
        "        objective='binary', # binary 문제\n",
        "        metric='auc',\n",
        "        learning_rate=.01,\n",
        "        n_estimators = int(n_estimators),\n",
        "        max_depth = int(maxDepth),\n",
        "        verbose=-1\n",
        "    )\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return np.mean(scores)\n",
        "\n",
        "def bayesOpt(x_train, y_train):\n",
        "    clfBO = BayesianOptimization(model_evaluate, {'n_estimators' : (100,200),\n",
        "                                                  'maxDepth' : (2,4)})\n",
        "    clfBO.maximize(init_points=5, n_iter=10)\n",
        "    print(clfBO.res)\n",
        "\n",
        "bayesOpt(x_train, y_train)\n",
        "\n",
        "# after finding out best parameters\n",
        "LGBM = LGBMClassifier(n_estimators=147,\n",
        "                    max_depth=4,\n",
        "                    random_state=1121)\n",
        "LGBM.fit(x_train, y_train)\n",
        "\n",
        "y_pred_train = LGBM.predict(x_train)\n",
        "y_pred_test = LGBM.predict(x_test)\n",
        "\n",
        "y_pred_train_proba = LGBM.predict_proba(x_train)[:, 1]\n",
        "y_pred_test_proba = LGBM.predict_proba(x_test)[:, 1]\n",
        "\n",
        "lgbm_re = pd.DataFrame({'model' : ['LGBM(BO)'],\n",
        "                      'f1_train' : metrics.f1_score(y_train, y_pred_train),\n",
        "                      'f1_test' : metrics.f1_score(y_test, y_pred_test),\n",
        "                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),\n",
        "                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba)})\n",
        "\n",
        "df_comparison = df_comparison.append(lgbm_re)\n",
        "df_comparison.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "XQaaGZzuTmXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking final result\n",
        "df_comparison.style.background_gradient(cmap='coolwarm', low=1)"
      ],
      "metadata": {
        "id": "ynFJHepRUKVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}