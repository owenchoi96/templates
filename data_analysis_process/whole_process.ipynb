{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 해결 프로세스\n",
        "\n",
        "#### 7 steps\n",
        "    1. Problem Definition (문제 정의)\n",
        "    2. Expected Effects\n",
        "    3. Solution\n",
        "    4. Prioritize\n",
        "    5. Data Analysis\n",
        "    6. Performance Monitoring\n",
        "    7. Model Operation        "
      ],
      "metadata": {
        "id": "WPPenL5mf1Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7 steps details\n",
        "    1. Problem Definition\n",
        "        * 과제 개요를 작성하는 단계로서 문제 현상을 '데이터'로 설명할 수 있도록 함\n",
        "        * 문제 현상과 문제 발생으로 인한 피해를 정량적으로 작성할 수 있으면 좋음 (back data 제시 good)\n",
        "        * Ex)\n",
        "            * 문제 현상 : 신규 고객 유입 후 재구매로 이어지는 고객 감소\n",
        "            * 예상 피해 : 재구매 고객 -> 충성 고객화 부진으로 매출성장 정체, 신규 고객 유입을 위한 마케팅 비용 증가로 인한 영업이익 감소\n",
        "            * back data?\n",
        "\n",
        "    2. Expected Effects\n",
        "        * 기대효과는 정성적이 아니라 정량적으로 작성할 수 있어야 함\n",
        "        * 이 단계에서 drop할 수도 있음\n",
        "        * Ex)\n",
        "            * 재구매 고객 증가 -> 충성 고객 증가 -> 매출 성장 -> 영업이익 증가\n",
        "            * 선순환 체계 구축으로 인한 서비스 성장\n",
        "            * go or drop?\n",
        "\n",
        "    3. Solution\n",
        "        * 얻을 수 있는 기대효과가 미미하다면 해당 과제의 drop을 고려할 수 있음\n",
        "        * 기대효과가 크고, 진행해야한다고 판단되는 과제라면, 해결하기 위한 방안을 탐색하고 list up함\n",
        "        * 해결방안은 꼭 모델링이 아니여도 됨 (간단한 통계분석으로도 해결 가능한 문제들이 있음)\n",
        "        * Ex)\n",
        "            * (1) EDA 및 일회성 데이터 분석을 통해 재구매 고객의 특성을 분석하고 이를 토대로 마케팅 기획\n",
        "            * (2) 재구매 가능성이 높은 고객을 예측하는 모델링 후 이를 활용한 타켓 마케팅\n",
        "\n",
        "    4. Prioritize\n",
        "        * list up 한 해결방안들에 대한 우선순위 결정\n",
        "        * 빠르게 수행할 수 있고, 결과를 파악할 수 있는 방법을 가장 최우선적으로 실행\n",
        "        * 1순위 방법론으로 해결이 어려울시를 대비하여, 2,3 순위 방법론도 준비\n",
        "        * Ex)\n",
        "            * (1)번을 빠르게 수행 후 파일럿 테스트 진행 및 성과 측정\n",
        "            * (1)번의 효과가 좋지 않다면, (2)번 진행 후 파일럿 테스트 진행\n",
        "\n",
        "    5. Data Analysis\n",
        "        * 결정된 우선 순위에 따라서 데이터 분석 및 모델링을 진행\n",
        "        * 분석 단계별 꼭 history 정리를 진행하고, 추후 동료들이 활용할 수 있도록 패키지화 하는 것도 중요\n",
        "\n",
        "    6. Performance Monitoring\n",
        "        * As-is vs. To-be 비교를 진행하며 정량적인 지표로 비교할 수 있도록 준비\n",
        "        * 최종 마케팅 후 성능을 평가하기 위한 지표 수립\n",
        "        * 분석 및 모델링을 통해 추출한 타켓 고객군(실험군)과 그렇지 않는 대조군을 설정하여 A/B test를 수행\n",
        "        * A/B test 결과 마케팅 반응률(구매 반응률) 비교를 통해 통계적으로 유믜미한지 검증 (t-test)\n",
        "        * 유의미한 결과를 얻을 때까지 파이럿 테스트를 수정하면서 진행\n",
        "        * marketing mix model을 통해 여러 조합을 계속 테스트\n",
        "\n",
        "    7. Model Operation\n",
        "        * 일,주,월 주기를 설정해야 하고, 주기적으로 모델 업데이트 일정도 고려해야 함\n",
        "        * Ex)\n",
        "            * 유의미한 결과가 발견되었다면 정규 마케팅으로 운영하기 위한 작업 준비\n",
        "            * 정해진 주기에 따라 타켓 고객 추출을 자동화하고, 이를 마케팅 시스템과 연계하여 타켓 마케팅을 주기적으로 운영하고 평가함"
      ],
      "metadata": {
        "id": "n8OmHy95cxnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Readiness Check & Sampling\n",
        "    (sampling -> computing resouce의 제한으로 인해 모든 데이터를 사용할 수 없는 경우에 사용)\n",
        "\n",
        "    1. Data Info Check\n",
        "    2. Data Readiness Check\n",
        "    3. Data Sampling\n",
        "\n",
        "    >> Details\n",
        "\n",
        "    1. Data Info Check\n",
        "        * 데이터의 전반적인 기본 정보를 파악하는 단계\n",
        "        * 기본 사항을 확인하며 전처리도 같이 수행\n",
        "\n",
        "        * check-list\n",
        "            1. data shape\n",
        "            2. data type\n",
        "            3. # of null values\n",
        "                * null value가 너무 많으면 삭제 고려\n",
        "            4. 중복 데이터\n",
        "                * pd.Series(df.dupicated()).value_counts()\n",
        "                * df[df.duplicated(keep=False)].sort_values(by=list(df.columns))\n",
        "                * df = df.drop_duplicates(keep='first')\n",
        "            5. outliers\n",
        "                * (현업가와의 커뮤니케이션을 통해 삭제할 지 놔둘지 결정할 필요가 있음)\n",
        "                \n",
        "\n",
        "    2. Data Readiness Check\n",
        "        * 데이터 수준 사전 점검\n",
        "            1. target label 생성\n",
        "                * Ex) 당월 구매 고객이 다음달 재구매 시 해당 고객을 재구매 고객으로 정의 (target 정의)\n",
        "            2. target ratio 확인\n",
        "                * 1번 단계에서 생성한 target ratio의 수준을 확인하고, 문제 해결이 가능한 수준인지 판단\n",
        "            3. 분석 방향성 결정\n",
        "                * 1,2번 단계를 종합하여 분석 방향성 결정\n",
        "                * target ratio가 낮다면 전체 고객이 아닌, 특정 segmentation 그룹으로 진행\n",
        "\n",
        "        * Ex) 재구매 고객을 target으로 선정할 시 (Target Label 생성)\n",
        "            * 기준년월 설정\n",
        "            * 데이터 적재 기간 확인\n",
        "            * 중복 데이터 제거\n",
        "                * 월별로 구매한 unique한 고객\n",
        "            * 기준년월 +1M\n",
        "                * (pd.to_datetime(i) + pd.DateOffset(months=1)).strftime('%Y-%m')\n",
        "                <codes>\n",
        "                    loop_list = list(df['bsym'].unique())\n",
        "                    df_all = []\n",
        "\n",
        "                    for i in loop_list:\n",
        "                        df_left = df[df['bsym'] == i]\n",
        "                        bsym_1m = (pd.to_datetime(i) + pd.DateOffset(months=1)).strftime('%Y-%m')\n",
        "                        df_right = pd.DataFrame(df[df['bsym'] == bsym_1m]['CustomerID'].unique())\n",
        "                        df_right['target'] = 1\n",
        "                        df_right.columns = ['CustomerID', 'target']\n",
        "                        df_merge = pd.merge(df_left, df_right, how='left')\n",
        "                        df_merge['target'] = df_merge['target'].fillna(0)\n",
        "\n",
        "                        df_all.append(df_merge)\n",
        "\n",
        "                    df_all = pd.concat(df_all)\n",
        "                * 월별 ratio, 전체 ratio같이 확인\n",
        "\n",
        "    3. Data Sampling\n",
        "        * 모든 데이터를 가지고 모델링 하는 것은 시간이 너무 오래 걸림\n",
        "        * sampling을 진행하면 효과적으로 데이터 분석 및 모델링 진행 가능 (under-sampling)\n",
        "            * 데이터 Mart 생성 시간 단축\n",
        "            * 모델 학습 시간 단축\n",
        "        * 현재 데이터의 상황에 맞는 sampling 기법을 사용하여 sampling 진행\n",
        "\n",
        "        * time-series data\n",
        "            * 주기를 변경할 때 많이 쓰임\n",
        "                (1) Up-sampling : 주기 변경시, 결측값 채우기\n",
        "                        >> df_time.resample('5S').mean()\n",
        "                    * 4 methods\n",
        "                        1. ffill\n",
        "                            >> df.fillna(method='ffill')\n",
        "                        2. bfill\n",
        "                            >> df.fillna(method='ffill')\n",
        "                        3. fill(0)\n",
        "                            >> df.fillna(0)\n",
        "                        4. mean()\n",
        "                            >> df.fillna(df.mean())\n",
        "                        5. linear interpolation 보간법 (가장 일반적)\n",
        "                            >> df.interpolate()\n",
        "\n",
        "                (2) Down-sampling : time window 기준 mean(), max(), min()\n",
        "                    >> df.resample('10S').mean()\n",
        "\n",
        "        * non time-series data\n",
        "            * target data가 너무 적으면 (clas-imbalance) -> over-sampling 진행\n",
        "            * data가 너무 많다면 under-sampling 진행\n",
        "                --> stratified sampling (층화추출) 진행\n",
        "\n",
        "            * over-sampling\n",
        "                * 대표적으로 SMOTE (synthetic minority over-sampling technique)\n",
        "                * 낮은 비율로 존재하는 클래스의 데이터를 최근접 이웃 (K-NN) 알고리즘 활용하여 새롭게 생성\n",
        "                <codes>\n",
        "                    from imblearn.over_sampling import SMOTE\n",
        "                    smote = SMOTE(random_state=42)\n",
        "                    X_over, y_over = smote.fit_resample(X,y)\n",
        "\n",
        "            * under-sampling & stratified sampling (층화추출)\n",
        "                * 모집단을 동질적인 소집단들로 층화시키고 그 집단의 크기에 따라 단순무작위 표본추출방법\n",
        "                * 장점  \n",
        "                    * 단순임의 추출보다 신뢰성이 높은 추정치를 구할 수 있음\n",
        "                    * 비용 절감 및 자료 분석이 용이\n",
        "                    * 각 층에 대한 추정치를 부수적으로 구할 수 있음\n",
        "                * 단점\n",
        "                    * 방대한 모집단의 경우 목록작성의 어려움\n",
        "                    * 적절하게 층을 나누기 위해서는 모집단 각 층에 대한 정확한 정보 필요\n",
        "                <codes>\n",
        "                df_target['sample_n_total'] = len(df_all) * .3 # 전체 데이터 셋에서 30% 비례 층화 추출\n",
        "                df_target['sample_n'] = df_target['sample_n_total'] * df_target['total_ratio']\n",
        "                df_target['sample_n'] = df_target['sample_n'].astype(int)\n",
        "\n",
        "                # 월별 비중 유지하면서 sampling\n",
        "                random_state = 1234\n",
        "                df_all_list = list()\n",
        "                for idx, bsym in enumerate(df_all['bsym'].unique()):\n",
        "                    df_ = df_all[df_all['bsym'] == bsym].sample(n=df_target['sample_n'][idx],\n",
        "                                                                random_state=random_state)\n",
        "                    df_all_list.append(df_)\n",
        "\n",
        "                df_all_sample = pd.concat(df_all_list)\n",
        "\n",
        "                # 층화추출이 정상적으로 되었는지 확인\n",
        "                df_all_sample_temp = df_all_sample.groupby(['bsym'])['target'].agg(['sum', 'count']).reset_index()\n",
        "                df_all_sample_temp['target_ratio'] = df_all_sample_temp['sum'] / df_all_sample_temp['count']\n",
        "                df_all_sample_temp['target_ratio_mosu'] = df_target['target_ratio']\n",
        "\n",
        "                df_all_sample_temp.style.bar(subset=['target_ratio', 'target_ratio_mosu'], align='zero', vmin=0, vmax=.8)\n",
        "                \n",
        "\n"
      ],
      "metadata": {
        "id": "EBn8ZOZpdO6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Mart & Feature engineering\n",
        "\n",
        "    1. Data Mart 기획 및 설계\n",
        "    2. Data 추출 및 Mart 개발\n",
        "    3. integer feature engineering\n",
        "    4. categorical feature engineering\n",
        "\n",
        "    >> Details\n",
        "\n",
        "    1. Data Mart 기획 및 설계\n",
        "    Data Sources --> Data Lake --> Data Warehouse --> Purchase Mart, Customer Mart, Online Mart\n",
        "\n",
        "        * sampling 완료 후 월별 기준이 되는 Customer ID를 추출한 상태 (해당 예시에서)\n",
        "        * Mart 구성하기 위한 준비가 모두 완료된 상태\n",
        "        * '가설을 수립'하고, 가설에 해당하는 다양한 category와 list를 생성\n",
        "            * logic까지 적어주어야 활용하기 편함\n",
        "        * 풍부한 data mart를 기획해야 좋은 모델의 성능과 다양한 해석이 가능\n",
        "\n",
        "    2. Data 추출 및 Mart 개발\n",
        "        * 상위 단계에서 정의한 Data Mart를 기반으로 변수를 추출하는 과정\n",
        "        * 변수 추출은 'sampling한 data'를 기반으로 진행\n",
        "\n",
        "        <example codes>\n",
        "        df_mart[['max_cnt', 'min_cnt']] = pd.DataFrame(df_origin_sample.groupby(['bsym', 'CustomerID', 'InvoiceNo'])['StockCode'].agg(stock_cnt='nunique').reset_index()).groupby(['bsym', 'CustomerID'])['stock_cnt'].agg(max_cnt=('max'), min_cnt=('min')).reset_index()[['max_cnt', 'min_cnt']]\n",
        "\n",
        "        df_mart['Country'] = df_origin_sample.groupby(['bsym', 'CustomerID'])['Country'].agg(country='first').reset_index()['country']\n",
        "\n",
        "    3. Integer Feature Engineering\n",
        "        * feature engineering?\n",
        "            --> 모델링의 목적인 성능향상을 위해 feature를 '생성, 선택, 가공'하는 일련의 모든 활동\n",
        "        \n",
        "        * target 변수와의 의미있는 변수 선택하는 방법\n",
        "            * regression (회귀) : 상관계수 분석을 통해 유의미한 변수 선택\n",
        "                * heatmap 등을 통해 파악 가능\n",
        "            * classification (분류) : bin(통)으로 구분 후 target 변수와의 관계 파악\n",
        "\n",
        "        * IV (Information Value)\n",
        "            * feature 하나가 good(target)과 bad(non-target)을 잘 구분해줄 수 있는지에 대한 정보량을 표현하는 방법\n",
        "            * IV 수치가 클수록 target과 non-target을 잘 구분할 수 있는 정보량이 많은 feature이고, IV 수치가 작을수록 정보량이 적은 feature\n",
        "            * (target data 구성비 - non-target data 구성비) * WoE\n",
        "            * WoE (Weight of Evidence) = ln(target data 구성비 / non-target data 구성비)\n",
        "\n",
        "    4. Categorical Feature Engineering\n",
        "        * target 변수와의 의미있는 변수 선택하는 방법\n",
        "        * 차원이 너무 많은 경우에는 domain 지식을 활용하여 차원 축소 추천\n",
        "            * catplot\n",
        "        * IV 수치 계산\n",
        "\n",
        "    -- IV 라이브러리 --\n",
        "    <codes>\n",
        "    !pip install -q optbinning\n",
        "\n",
        "    feature_list = []\n",
        "    iv_df = []\n",
        "\n",
        "    for i in feature_list:\n",
        "        variable = i\n",
        "        x = df_mart[variable].values\n",
        "        y = df_mart.target\n",
        "\n",
        "        optb = OptimalBinning(name=variable,\n",
        "                              dtype=dtype, # numerical, categorical\n",
        "                              solver=solver, # cp, mip\n",
        "                              max_n_prebins=3, # when numerical\n",
        "                              cat_cutoff=.1 # when categorical\n",
        "                              )\n",
        "        optb.fit(x,y)\n",
        "        # print('split point : ', optb.splits)\n",
        "\n",
        "        binnging_table = optb.binning_table\n",
        "        v1 = binning_table.build()\n",
        "        # display(binning_table.build())\n",
        "        # binning_table.plot(metric='event_rate')\n",
        "\n",
        "        df = pd.DataFrame({'val' : variable,\n",
        "                           'IV' : [v1.loc['Totals', 'IV']]})\n",
        "        iv_df.append(df)\n",
        "\n",
        "    iv_df = pd.concat(iv_df).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "odyWBlIugeIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Modeling & Evaluation\n",
        "\n",
        "    1. 모델링을 위한 데이터 사전 준비\n",
        "    2. Model Selection\n",
        "    3. 모델링 및 성능비교\n",
        "    4. Model Evaluation\n",
        "\n",
        "    Details >>\n",
        "\n",
        "    1. 모델링을 위한 데이터 사전 준비\n",
        "        * 모델이 이해할 수 있는 데이터의 형태로 가공\n",
        "            * 모델은 숫자로 이루어진 형태의 data만 인식 가능\n",
        "            * 모델링을 수행하기 위해 feature와 예측하고자 하는 y로 데이터를 나눔 (X, y)\n",
        "            * train, test split\n",
        "        \n",
        "        * 대표적인 encoding 방법\n",
        "            * one-hot encoding\n",
        "                * 변수의 차원이 늘어나는 효과\n",
        "                * 예측 성능의 저하\n",
        "                * 범주형 변수의 unique한 수준을 축소하고 사용하기를 권장\n",
        "            * label-encoding\n",
        "                * 숫자의 ordinal한 특성이 반영되어 의도하지 않은 관계성이 생김\n",
        "                * 예측 성능의 저하\n",
        "                * 선형회귀와 같은 ML 알고리즘에 적용 X\n",
        "                * Tree 계열의 알고리즘은 숫자의 ordinal한 특성을 반영하지 않으므로 진행 가능\n",
        "\n",
        "    2. Model Selection\n",
        "        * 학습할 모델 list up\n",
        "            * 대표적인 알고리즘에 대해서는 모두 test하는 것이 유리\n",
        "            * 분석하고자 하는 데이터의 특성이 모두 다르므로 완벽한 알고리즘은 없음\n",
        "            * data by data\n",
        "            * '동일한 작동원리의 알고리즘을 사용하기보다는, 다른 작동원리를 가지고 있는 다양한 알고리즘 선정'\n",
        "            \n",
        "        * Regression\n",
        "            * 선형 모델 (Ridge, Lasso, Elastic Net)\n",
        "            * 비선형 회귀 모델 (Polynomial, log 모형)\n",
        "            * Tree 계열\n",
        "                * Bagging 앙상블 (RandomForest) : 아직까지도 성능이 좋음\n",
        "                * Boosting 앙상블 (LightGBM) : 가벼워서 학습하는데 시간이 덜 듬\n",
        "\n",
        "        * Classification\n",
        "            * 로지스틱 (Logistic Regression)\n",
        "            * Tree 계열 classification 모델\n",
        "                * Bagging 앙상블 (Random Forest)\n",
        "                * Boosting 앙상블 (LightGBM)\n",
        "\n",
        "        * 주의사항\n",
        "            * 하이퍼 파라미터 튜닝 이후 성능이 더 안나오면 Data Mart로 올라가 업데이트 하고 다시 모델을 돌려보는 것을 추천\n",
        "            * AutoML이 모델링을 다 해주기 때문에 feature에 더욱 집중하는 것이 좋을 수 있음\n",
        "    \n",
        "        <정리>\n",
        "        1. 작동원리가 다른 모델들을 돌려보고,\n",
        "        2. 성능이 안나오면 feature mart로 다시 올라가고,\n",
        "        3. 하이퍼 파라미터 튜닝을 하면서 조절하고,\n",
        "        4. 다른 알고리즘을 알아볼 시간이 있다면 다른 알로리즘 서치 및 적용해보고\n",
        "\n",
        "    3. 모델링 및 성능 비교\n",
        "        * model 학습\n",
        "            * model selection 단계에서 선정한 모델들을 학습하고 성능을 기록\n",
        "            * 동일한 Dataset, 동일한 환경에서 동일한 비교 지표로 성능을 비교\n",
        "\n",
        "        * 성능평가\n",
        "            * Regression\n",
        "                * MAE\n",
        "                * MSE : 오차의 민감도를 높이는 효과\n",
        "                * RMSE\n",
        "                * R2 : 결정계수, 관측값의 분산대비 예측값의 분산을 비교하여 모델의 정확도 성능을 측정, 0~1까지 표현\n",
        "            * Classification\n",
        "                * Precision : 정답이라고 예측한 것 들 중에 실제 정답인 것의 비율\n",
        "                * Recall : 실제 정답인 것 중에 모델이 정답이라고 예측한 것으 비율\n",
        "                * F1-score : 모델의 label이 불균형할 때 모델의 성능을 객관적으로 평가할 수 있음\n",
        "                * AUC, AUROC : 과적합이 되었는지 확인가능\n",
        "                    >> from sklearn.metrics import roc_auc_score\n",
        "\n",
        "                        y_pred_train_proba = LR.predict_proba(x_train_sc)[:, 1]\n",
        "                        y_pred_test_proba = LR.predict_proba(x_test_sc)[:, 1]\n",
        "\n",
        "                        roc_score_train = roc_auc_score(y_train, y_pred_train_proba)\n",
        "                        roc_score_test = roc_auc_score(y_test, y_pred_test_proba)\n",
        "\n",
        "                        # 둘의 성능차이를 보고 하이퍼 파라미터 조율 가능\n",
        "                        print('roc_score_train : ', roc_score_train)\n",
        "                        print('roc_score_test : ', roc_score_test)\n",
        "                * classification_report\n",
        "                    >> from sklearn.metrics import classification_report\n",
        "\n",
        "    4. Model Evaluation\n",
        "        * 전체 모델 성능 평가\n",
        "            * hyper-parameter tunning 전 전체 모델 성능 비교\n",
        "        \n",
        "        * 과적합\n",
        "            * 과적합이 발생하면, Robust(<->Sensitive) 한 모델이 될 수 없고, 모델 운영시 일정한 성능을 유지할 확률이 낮음\n",
        "\n",
        "        * 과적합 해결방법\n",
        "            * 현업에서는 하이퍼파라미터 조절을 통한 규제로 과적합을 방지\n",
        "            * tree model을 예시로 max_depth 값을 작은 값으로 설정하여 모델이 학습 데이터를 덜 학습하게 만듬\n",
        "            * 더 Robust한 모델을 만들기 위해 train/test data를 활용하여 하이퍼파라미터 튜닝하고, validation set으로 최종 성능평가 진행\n",
        "            * train / validation / test의 성능 차이의 gap을 최소화하면서 성능을 향상시킬 수 있는 튜닝 진행"
      ],
      "metadata": {
        "id": "tGKRNw1BwtBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Tunning & Operation\n",
        "\n",
        "    * Hyper-parameter Tunning\n",
        "    * Model Explanation\n",
        "    * Model Operation\n",
        "\n",
        "    Details >>\n",
        "\n",
        "    1. Hyper-parameter Tunning\n",
        "        * parameter : 모델 내부에서 정해지는 변수 ex) 회귀식에서 회귀계수\n",
        "        * hyper-parameter : 최적의 parameter를 도출하기 위해 직접 세팅하는 값\n",
        "\n",
        "        * 대표적인 방법론\n",
        "            * grid-search\n",
        "            * random search\n",
        "            * BayseianOptimization\n",
        "                * 입력값 (x)를 input으로 목적 함수 f(x)를 찾는 것\n",
        "                * x (hyper-parameter) / f(x) (precision, recall, AUC 등)\n",
        "                * 순차적으로 하이퍼 파라미터를 업데이트해 가면서 평가를 통해 최적의 하이퍼파라미터 조합을 탐색\n",
        "        <codes>\n",
        "        # !pip install -q bayesian-optimization\n",
        "        from bayes_opt import BayesianOptimization\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "\n",
        "        def model_evaluate(n_estimators, max_depth):\n",
        "            # RandomForest\n",
        "            clf = RandomForestClasifier(\n",
        "                n_estimators = int(n_estimators),\n",
        "                max_depth = int(max_depth)\n",
        "            )\n",
        "            scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "            return np.mean(scores)\n",
        "\n",
        "            # LGBM\n",
        "            clf = LGBMClassifier(\n",
        "                objective='binary',\n",
        "                metric='auc',\n",
        "                learning_rate=.01,\n",
        "                n_estimators = int(n_estimators),\n",
        "                max_depth = int(maxDepth)\n",
        "            )\n",
        "\n",
        "        def bayesOpt(x_train, y_train):\n",
        "            clf_bo = BayesianOptimization(model_evaluate, {'n_estimators' : (100,200),\n",
        "                                                            'max_depth' : (2,4)})\n",
        "            clf_bo.maximize(init_points=5, n_iter=10)\n",
        "            print(clf_bo.res) # 결과 확인 가능\n",
        "\n",
        "        bayesOpt(x_train, y_train)\n",
        "\n",
        "    2. Model Explanation\n",
        "        * Permutation Importance (<-> Feature Importance)\n",
        "            * feature와 실제 결과값 간의 관계 (연결고리)를 끊어내도록 특성들의 값을 '랜덤하게 섞은 후' 모델 예측치의 오류 증가량을 측정,\n",
        "            * 만약 하나의 특성을 무작위로 (shuffle) 섞었을 때, 모델 오류가 증가한다면 모델이 예측할 때 해당 특성에 의존한다는 것을 의미\n",
        "            * 모델의 성능이 떨어지면, 중요한 feature\n",
        "            * 모델의 성능이 그대로이거나, 좋아지면 중요하지 않은 feature\n",
        "\n",
        "            * 알고리즘에 상관없이 다 돌릴 수 있음\n",
        "            <codes>\n",
        "            # !pip install -q eli5\n",
        "            import eli5\n",
        "            from eli5.sklearn import PermutationImportance\n",
        "\n",
        "            # LR model\n",
        "            perm = PermutationImportance(LR, random_state=1).fit(x_train_sc, y_train) # >> random_state 바꿔가며 테스트 해봐야 됨\n",
        "            eli5.show_weights(perm, feature_names = x_train.columns.tolist()) # 상호관계를 고려하고 난 후의 점수\n",
        "        \n",
        "        * Shapley Value\n",
        "            * 특정 feature가 예측값에 얼마나 기여하는지 파악하기 위해 특정 변수와 관련된 모든 변수 조합들을 입력시켰을 때,\n",
        "            * 나온 결과값과 비교를 하면서 변수의 기여도를 계산하는 방식, 즉 특정 Feature (on/off)가 예측값에 얼마나 영향을 끼쳤는지 탐색\n",
        "            * 데이터 객체 각각의 instance 마다 왜 이런 결과가 나왔는지 확인할 수도 있음\n",
        "            <codes>\n",
        "            # !pip install -q shap\n",
        "            import shap\n",
        "\n",
        "            # summary_plot (globally)\n",
        "            explainer = shap.LinearExplainer(LR.fit(x_train_sc, y_train), x_train_sc)\n",
        "            shap_values = explainer.shap_values(x_test_sc)\n",
        "            shap.summary_plot(shap_values, x_test_sc, feature_names = x_test.columns, plot_type='bar', class_names=y_test)\n",
        "\n",
        "            # force_plot (locally)\n",
        "            shap.initjs()\n",
        "            shap.force_plot(explainer.expected_value, shap_values[0], features=x_test_sc[0], feature_names=x_test.columns, link='logit')\n",
        "\n",
        "        * 두가지를 같이 쓰는 이유?\n",
        "            * 두가지에서 높은 순위를 나타내는 피처들이 있고 중복이 된다면 더 높은 신뢰성을 확보할 수 있음\n",
        "\n",
        "\n",
        "    3. Model Operation\n",
        "        * 최종모델 활용 운영 준비\n",
        "            1. target 고객 수준 결정 : 최종 selection된 model 기준으로 재구매 가능성이 높은 target 고객군 선정\n",
        "            2. model save : model operation을 위해 모델 및 하이퍼 파라미터 저장 후 운영시 model 및 하이퍼 파라미터 load\n",
        "            3. mart 생성 : mart를 생성할 때 사용했던 전처리 및 완료 코드를 저장 후 운영 시 불러와 월별 새로운 mart 생성\n",
        "            4. model input : 생성된 mart를 model에 input하여 결과를 추출 (proba 예측확률)\n",
        "            5. target 추출 : 1단계에서 선정한 threshold 기준으로 target 고객 추출\n",
        "\n",
        "            <1번 codes>\n",
        "            bins=[.0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n",
        "            lift_base = y_train.value_counts(normalize=True)[1]\n",
        "\n",
        "            confusion_matrix1 = pd.crosstab(pd.cut(y_pred_train_proba, bins, right=False), y_train, rownames=['Predicted'], colnames=['Actual'], margins=True)\n",
        "\n",
        "            # confusion_matrix1 = pd.crosstab(pd.qcut(y_pred_train_proba, 10), y_train, rownames=['Predicted'], colnames=['Actual'], margins=True)\n",
        "            confusion_matrix1['ratio'] = round((pd.DataFrame(confusion_matrix1)[1]/pd.DataFrame(confusion_matrix1)['All']), 2)\n",
        "            confusion_matrix1['Lift'] = round(confusion_matrix1['ratio']/lift_base, 1)\n",
        "            confusion_matrix1\n",
        "\n",
        "            <2번 codes>\n",
        "            import pickle\n",
        "\n",
        "            saved_model = pickle.dumps(rfc) # 모델 저장\n",
        "            rfc = pickle.loads(saved_model) # 모델 read\n"
      ],
      "metadata": {
        "id": "nApUpUwBzcRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6l9AJ0x2qL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Interpretation"
      ],
      "metadata": {
        "id": "q5JMEOIfx8i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    1. lightgbm으로 모델 생성\n",
        "    2. SHAP으로 방향성 및 변수간 상호작용을 보고 중요변수 N개 추출\n",
        "    3. 중요변수 N개를 사용해 depth=3,4까지 해서 DecisionTree를 만듬\n",
        "    4. Rule Extraction --> 해석을 가능하게 함."
      ],
      "metadata": {
        "id": "KaDrGsh9yBEz"
      }
    }
  ]
}