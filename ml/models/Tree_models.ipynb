{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMdv9Qp4-yBf"
      },
      "outputs": [],
      "source": [
        "# decision tree\n",
        "# bagging\n",
        "    # random forests\n",
        "# boosting\n",
        "    # xgboost\n",
        "    # lightgbm\n",
        "\n",
        "# common libraries\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn import tree # for rule extraction\n",
        "# Random Forests\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "# AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "# LightGBM\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "\n",
        "# data\n",
        "data = pd.read_csv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparation\n",
        "Y = data[]\n",
        "X = data.drop(columns=[])\n",
        "\n",
        "print('Y shape : ', Y.shape)\n",
        "print('X shape : ', X.shape)\n",
        "\n",
        "# spliting data\n",
        "idx = list(range(X.shape[0]))\n",
        "train_idx, valid_idx = train_test_split(idx, test_size=.3, random_state=)\n",
        "print('# of train data : ', len(train_idx))\n",
        "print('# of valid data : ', len(valid_idx))\n",
        "print('# of train data Y : ', Counter(Y.iloc[train_idx]))\n",
        "print('# of valid data Y : ', Counter(Y.iloc[valid_idx]))"
      ],
      "metadata": {
        "id": "zL9YJMlCAMSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- decision tree --\n",
        "# max_depth : 2~5\n",
        "# f1_score가 떨어지면 overfit 암시\n",
        "\n",
        "# Depth 조절 Decision Tree\n",
        "# f1_score가 떨어지면 overfitting되었다는 소리\n",
        "\n",
        "for i in range(2, 5+1, 1):\n",
        "    print('--- depth {} ---'.format(i))\n",
        "\n",
        "    model = DecisionTreeClassifier(max_depth=i, criterion='gini')\n",
        "    model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "    # Train Acc\n",
        "    y_pre_train = model.predict(X.iloc[train_idx])\n",
        "    cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "    print('Train Confusion Matrix')\n",
        "    print(cm_train)\n",
        "    print('Train Acc : {}'.format((cm_train[0, 0] + cm_train[1,1])/cm_train.sum()))\n",
        "    print('Train F1 score : {}\\n'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "    # Test Acc\n",
        "    y_pre_train = model.predict(X.iloc[valid_idx])\n",
        "    cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_train)\n",
        "    print('Valid Confusion Matrix')\n",
        "    print(cm_train)\n",
        "    print('Valid Acc : {}'.format((cm_train[0, 0] + cm_train[1,1])/cm_train.sum()))\n",
        "    print('Valid F1 score : {}\\n'.format(f1_score(Y.iloc[valid_idx], y_pre_train)))\n",
        "\n",
        "# best model\n",
        "best_model = DecisionTreeClassifier(max_depth=, criterion='gini')\n",
        "best_model.fit(X.iloc[train_idx], Y,iloc[train_idx])\n",
        "\n",
        "# tree plot\n",
        "plt.rcParams['figure.figsize'] = [20,10]\n",
        "tree.plot_tree(model, filled=True, feature_names=X.columns,\n",
        "               class_names = [])"
      ],
      "metadata": {
        "id": "PQ9bzcz-AKIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- random forests --\n",
        "\n",
        "# parameters\n",
        "    # n_estimators : # of trees\n",
        "    # max_depth\n",
        "    # criterion : gini, entropy, log_loss\n",
        "    # min_samples_split : 2개 이상으로 바꿀 수 있음\n",
        "    # bootstrap\n",
        "    # max_features : auto, sqrt, log2\n",
        "    # oob_score : out-of-bag score\n",
        "    # class_weight : label imbalance 데이터 학습 시 weight 조절\n",
        "    # random_state\n",
        "\n",
        "estimators = [10, 30, 40, 50, 60]\n",
        "depth = [4, 5, 10, 15]\n",
        "\n",
        "# modeling\n",
        "save_est = []\n",
        "save_dep = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "for est in estimators:\n",
        "    for dep in depth:\n",
        "        print('--- cnt {} ---'.format(cnt))\n",
        "        cnt+=1\n",
        "        print('Number of Estimators : {}, Max Depth : {}'.format(est,dep))\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=est,\n",
        "                                      max_depth=dep,\n",
        "                                      random_state=,\n",
        "                                      criterion='gini',\n",
        "                                      max_features='auto',\n",
        "                                      bootstrap=True,\n",
        "                                      oob_score=True) # oob_score=True -> longer time for training\n",
        "        model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "        # Train Acc\n",
        "        y_pre_train = model.predict(X.iloc[train_idx])\n",
        "        cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "        print('Train Confusion Matrix')\n",
        "        print(cm_train)\n",
        "        print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "        print('Train F1 score : {}'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "        # Test Acc\n",
        "        y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "        cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "        print('Valid Confusion Matrix')\n",
        "        print(cm_train)\n",
        "        print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "        print('Valid F1 score : {}\\n'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "\n",
        "        save_est.append(est)\n",
        "        save_dep.append(dep)\n",
        "        f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))"
      ],
      "metadata": {
        "id": "MUtfhdq8BRsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- AdaBoost --\n",
        "\n",
        "# parameters\n",
        "    # n_estimators : # of trees\n",
        "    # learning_rate : learning_rate와 n_estimators는 trade-off 관계에 있음\n",
        "\n",
        "estimators = [70, 90, 100]\n",
        "learning_rate = [.01, .03, .05, .1, .5]\n",
        "\n",
        "# modeling\n",
        "save_est = []\n",
        "save_lr = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "for est in estimators:\n",
        "    for lr in learning_rate:\n",
        "        print('>>> cnt {} <<<'.format(cnt))\n",
        "        cnt+=1\n",
        "        print('Number of Estimators : {}, Max Depth : {}'.format(est, lr))\n",
        "\n",
        "        model = AdaBoostClassifier(n_estimators=est, learning_rate=lr, random_state=119)\n",
        "        model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "        # Train Acc\n",
        "        y_pre_train = model.predict(X.iloc[train_idx])\n",
        "        cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "        print('Train Confusion Matrix')\n",
        "        print(cm_train)\n",
        "        print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "        print('Train F1 score : {}\\n'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "        # Test Acc\n",
        "        y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "        cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "        print('Valid Confusion Matrix')\n",
        "        print(cm_train)\n",
        "        print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "        print('Valid F1 score : {}'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "        print('-'*60)\n",
        "\n",
        "        save_est.append(est)\n",
        "        save_lr.append(lr)\n",
        "        f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))"
      ],
      "metadata": {
        "id": "vo3PgJ2cC_Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- gradient boosting --\n",
        "\n",
        "# parameters\n",
        "    # n_estimators : # of trees\n",
        "    # learning_rate : n_estimators와 trade-off에 있음\n",
        "    # max_features : feature수 sampling\n",
        "    # subsample : data subsample (bootstrap X)\n",
        "    # max_depth : tree 최대 깊이 제한\n",
        "\n",
        "estimators = [10, 20, 50]\n",
        "learning_rate = [.05, .1, .5]\n",
        "subsample = [.5, .75, 1]\n",
        "\n",
        "# modeling\n",
        "save_est = []\n",
        "save_lr = []\n",
        "save_sub = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "for est in estimators:\n",
        "    for lr in learning_rate:\n",
        "        for sub in subsample:\n",
        "            print('>>> cnt {} <<<'.format(cnt))\n",
        "            cnt+=1\n",
        "            print('Number of Estimators : {}, Learning Rate : {}, Subsample : {}'.format(est, lr, sub))\n",
        "\n",
        "            model = GradientBoostingClassifier(n_estimators=est,\n",
        "                                               learning_rate=lr,\n",
        "                                               subsample=sub,\n",
        "                                               random_state=)\n",
        "            model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "            # Train Acc\n",
        "            y_pre_train = model.predict(X.iloc[train_idx])\n",
        "            cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "            print('Train Confusion Matrix')\n",
        "            print(cm_train)\n",
        "            print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "            print('Train F1 score : {}\\n'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "            # Test Acc\n",
        "            y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "            cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "            print('Valid Confusion Matrix')\n",
        "            print(cm_train)\n",
        "            print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "            print('Valid F1 score : {}'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "            print('-'*60)\n",
        "\n",
        "            save_est.append(est)\n",
        "            save_lr.append(lr)\n",
        "            save_sub.append(sub)\n",
        "            f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))"
      ],
      "metadata": {
        "id": "FFIFeuM9EsyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- xgboost --\n",
        "\n",
        "# objective[default=reg:linear]\n",
        "    # binary:logistic\n",
        "    # multi:softmax\n",
        "    # multi:softprob\n",
        "# hyperparameters tuning\n",
        "    # n_estimators, learning_rate, max_depth, reg_alpha\n",
        "\n",
        "n_tree = [5, 10, 20]\n",
        "l_rate = [.1, .3]\n",
        "m_depth = [3, 5]\n",
        "L1_norm = [.1, .3, .5]\n",
        "\n",
        "# modeling\n",
        "save_n = []\n",
        "save_l = []\n",
        "save_m = []\n",
        "save_L1 = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "for n in n_tree:\n",
        "    for l in l_rate:\n",
        "        for m in m_depth:\n",
        "            for L1 in L1_norm:\n",
        "                print('--- cnt {} ---'.format(cnt))\n",
        "                cnt+=1\n",
        "                print('Number of Estimators : {}, Learning Rate : {}, '\\\n",
        "                      'Max Depth : {}, L1 Norm : {}'.format(n, l, m, L1))\n",
        "\n",
        "                model = XGBClassifier(n_estimators=n,\n",
        "                                     learning_rate=l,\n",
        "                                     max_depth=m,\n",
        "                                     reg_alpha=L1,\n",
        "                                     objective=, # dependent to task\n",
        "                                     random_state=)\n",
        "                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "                # Train Acc\n",
        "                y_pre_train = model.predict(X.iloc[train_idx])\n",
        "                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "                print('Train Confusion Matrix')\n",
        "                print(cm_train)\n",
        "                print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "                print('Train F1 score : {}\\n'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "                # Test Acc\n",
        "                y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "                cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "                print('Valid Confusion Matrix')\n",
        "                print(cm_train)\n",
        "                print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "                print('Valid F1 score : {}'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "                print('-'*60)\n",
        "\n",
        "                save_n.append(n)\n",
        "                save_l.append(l)\n",
        "                save_m.append(m)\n",
        "                save_L1.append(L1)\n",
        "                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n",
        "\n",
        "                # saving model\n",
        "                # joblib.dump(model, 'sample_data/XGBoost_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(f1_score_[-1], 4)))\n",
        "                # gc.collect()"
      ],
      "metadata": {
        "id": "MWPm_VOyF0_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- lightgbm --\n",
        "n_tree = [5, 10, 20]\n",
        "l_rate = [.1, .3]\n",
        "m_depth = [3, 5]\n",
        "L1_norm = [.1, .3, .5]\n",
        "\n",
        "# modeling\n",
        "save_n = []\n",
        "save_l = []\n",
        "save_m = []\n",
        "save_L1 = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "for n in n_tree:\n",
        "    for l in l_rate:\n",
        "        for m in m_depth:\n",
        "            for L1 in L1_norm:\n",
        "                print('>>> cnt {} <<<'.format(cnt))\n",
        "                cnt+=1\n",
        "                print('Number of Estimators : {}, Learning Rate : {}, '\\\n",
        "                      'Max Depth : {}, L1 Norm : {}'.format(n, l, m, L1))\n",
        "\n",
        "                model = LGBMClassifier(n_estimators=n,\n",
        "                                      learning_rate=l,\n",
        "                                      max_depth=m,\n",
        "                                      reg_alpha=L1,\n",
        "                                      objective='cross_entropy', # 설정 필요\n",
        "                                      n_jobs=-1,\n",
        "                                      random_state=119)\n",
        "                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "                # Train Acc\n",
        "                y_pre_train = model.predict(X.iloc[train_idx])\n",
        "                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "                print('Train Confusion Matrix')\n",
        "                print(cm_train)\n",
        "                print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "                print('Train F1 score : {}\\n'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "                # Test Acc\n",
        "                y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "                cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "                print('Valid Confusion Matrix')\n",
        "                print(cm_train)\n",
        "                print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "                print('Valid F1 score : {}'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "                print('-'*60)\n",
        "\n",
        "                save_n.append(n)\n",
        "                save_l.append(l)\n",
        "                save_m.append(m)\n",
        "                save_L1.append(L1)\n",
        "                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n",
        "\n",
        "                # saving model\n",
        "                # joblib.dump(model, 'sample_data/XGBoost_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(f1_score_[-1], 4)))\n",
        "                # gc.collect()"
      ],
      "metadata": {
        "id": "3WaXkbUQHvsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- getting best model & plotting feature importances --\n",
        "\n",
        "# choosing best model\n",
        "# random forests\n",
        "best_model = RandomForestClassifier(n_estimators=save_est[np.argmax(f1_score_)],\n",
        "                                    max_depth=save_dep[np.argmax(f1_score_)],\n",
        "                                    random_state=119,\n",
        "                                    criterion='gini',\n",
        "                                    max_features='auto',\n",
        "                                    bootstrap=True,\n",
        "                                    oob_score=False)\n",
        "# adaboost\n",
        "best_model = AdaBoostClassifier(n_estimators=save_est[np.argmax(f1_score_)],\n",
        "                                learning_rate=save_lr[np.argmax(f1_score_)],\n",
        "                                random_state=)\n",
        "# gradient boosting\n",
        "best_model = GradientBoostingClassifier(n_estimators=save_est[np.argmax(f1_score_)],\n",
        "                                        learning_rate=save_lr[np.argmax(f1_score_)],\n",
        "                                        subsample=save_sub[np.argmax(f1_score_)],\n",
        "                                        random_state=)\n",
        "# xgboost\n",
        "best_model = XGBClassifier(n_estimators=save_n[np.argmax(f1_score_)],\n",
        "                        learning_rate=save_l[np.argmax(f1_score_)],\n",
        "                        max_depth=save_m[np.argmax(f1_score_)],\n",
        "                        reg_alpha=save_L1[np.argmax(f1_score_)],\n",
        "                        objective=,\n",
        "                        random_state=)\n",
        "# lightgbm\n",
        "best_model = LGBMClassifier(n_estimators=save_n[np.argmax(f1_score_)],\n",
        "                            learning_rate=save_l[np.argmax(f1_score_)],\n",
        "                            max_depth=save_m[np.argmax(f1_score_)],\n",
        "                            reg_alpha=save_L1[np.argmax(f1_score_)],\n",
        "                            objective='cross_entropy', # 설정 필요\n",
        "                            n_jobs=-1,\n",
        "                            random_state=)\n",
        "\n",
        "\n",
        "# fitting best model\n",
        "best_model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "# Train Acc\n",
        "y_pre_train = best_model.predict(X.iloc[train_idx])\n",
        "cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "print('Train Confusion Matrix')\n",
        "print(cm_train)\n",
        "print('Train Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "print('Train F1 score : {}'.format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "# Test Acc\n",
        "y_pre_test = best_model.predict(X.iloc[valid_idx])\n",
        "cm_train = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "print('Valid Confusion Matrix')\n",
        "print(cm_train)\n",
        "print('Valid Acc : {}'.format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "print('Valid F1 score : {}\\n'.format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "\n",
        "# feature importance\n",
        "feature_map = pd.DataFrame(sorted(zip(best_model.feature_importances_, X.columns), reverse=True), columns=['Score', 'Feature'])\n",
        "print(feature_map)\n",
        "\n",
        "# feature map\n",
        "feature_map_20 = feature_map.iloc[:10]\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(x='Score', y='Feature',\n",
        "            data=feature_map_20.sort_values(by='Score', ascending=False),\n",
        "            errwidth=40\n",
        "            )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S6sqmFiGD70m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}