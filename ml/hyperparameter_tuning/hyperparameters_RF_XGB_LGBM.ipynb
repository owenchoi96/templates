{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52rXK26z2xqE"
      },
      "outputs": [],
      "source": [
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc = RandomForestClassifier(\n",
        "    n_estimators=, # 결정 트리 갯수\n",
        "    max_depth=, # 트리의 최대 깊이\n",
        "    min_samples_split=, # 노드를 분할하기 위한 최소한의 샘플 데이터수 (과적합 제어), 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 up\n",
        "    min_samples_leaf=, # 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수 (과적합 제어)\n",
        "    max_features=, # 최적의 분할을 위해 고려할 최대 feature 개수 // int: 피처갯수, float: 비중, sqrt(auto): 전체 피처중 sqrt 만큼 선정, log: log2만큼\n",
        "    max_leaf_nodes=, # 리프노드의 최대 개수\n",
        "    criterion=, # 'gini', 'entropy', 'log_loss'\n",
        "    random_state=,\n",
        "    n_jobs=,\n",
        "    verbose=,\n",
        "    class_weight=,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    # general parameters\n",
        "    silent=, # default=1\n",
        "    nthread=, # cpu 실행 스레드 개수 조정\n",
        "\n",
        "    # booster paramters\n",
        "    min_child_weight=, # leaf node에 포함되는 최소 관측치의 수 (min_samples_split from rfc)\n",
        "    gamma=, # leaf node의 추가분할을 결정할 최소손실 감소값, 해당값보다 손실이 크게 감소할때 분리, (과적합 제어)\n",
        "    max_depth=, # 과적합 제어\n",
        "    sub_sample=, # 학습시 데이터 샘플링 비율 지정 (과적합 제어), 일반적으로 0.5~1\n",
        "    colsample_bytree=, # 트리 생성에 필요한 feature의 샘플링에 사용, feature가 많을 때 사용\n",
        "    reg_lambda= , # L2_Regularization\n",
        "    reg_alpha= , # L1_Regularization\n",
        "    scale_pos_weight= , # 불균형 데이터셋의 균형을 유지\n",
        "    eta=, # learning_rate\n",
        "\n",
        "    # train parameters\n",
        "    objective=, # reg:linear(회귀), binary:logistic(이진), multi:softmax(다중분류,클래스반환), multi:softprob(다중분류,확률반환)\n",
        "    eval_metric=, # rmse, mae, logloss, error(binary), merror(multiclass), mlogloss(multiclass logloss), auc (area under curve)\n",
        ")"
      ],
      "metadata": {
        "id": "qXPlLm3R4TDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lightGBM\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(\n",
        "    # basic ones\n",
        "    max_depth=,\n",
        "    learning_rate=,\n",
        "    n_estimators=,\n",
        "\n",
        "    # others\n",
        "    num_leaves=,\n",
        "    subsample_for_bin=,\n",
        "    objective=, # 'regression', 'binary', 'multiclass', 'lambdarank'\n",
        "    class_weight=, # only for multiclass classification, use 'scale_pos_weight' or 'is_unbalance' for binary classification\n",
        "    min_child_weight=,\n",
        "    min_child_samples=, # minimum number of data needed in a child (leaf)\n",
        "    subsample=, # subsample ratio\n",
        "    colsample_bytree=, # subsample ratio of columns\n",
        "    reg_alpha=, # L1\n",
        "    reg_lambda=, # L2\n",
        "    random_state=,\n",
        "    n_jobs=,\n",
        ")"
      ],
      "metadata": {
        "id": "zBnGv2Ky_gjM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}